{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"starGAN_v2_celeba_hq.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["import os\n","import sys\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"43iJMR6l1eTx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_root = '/content/drive/MyDrive/GAN/starGAN_v2'\n","%cd /content/drive/MyDrive/GAN/starGAN_v2"],"metadata":{"id":"dTaap3pU2S6g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# download.sh 생성 및 아래 text 내용 저장\n","#!touch download.sh"],"metadata":{"id":"-MNsUWqZ1xGw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\"\"\"\n","StarGAN v2\n","Copyright (c) 2020-present NAVER Corp.\n","\n","This work is licensed under the Creative Commons Attribution-NonCommercial\n","4.0 International License. To view a copy of this license, visit\n","http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n","Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n","\"\"\"\n","\n","FILE=$1\n","\n","if [ $FILE == \"pretrained-network-celeba-hq\" ]; then\n","    URL=https://www.dropbox.com/s/96fmei6c93o8b8t/100000_nets_ema.ckpt?dl=0\n","    mkdir -p ./expr/checkpoints/celeba_hq\n","    OUT_FILE=./expr/checkpoints/celeba_hq/100000_nets_ema.ckpt\n","    wget -N $URL -O $OUT_FILE\n","\n","elif  [ $FILE == \"pretrained-network-afhq\" ]; then\n","    URL=https://www.dropbox.com/s/etwm810v25h42sn/100000_nets_ema.ckpt?dl=0\n","    mkdir -p ./expr/checkpoints/afhq\n","    OUT_FILE=./expr/checkpoints/afhq/100000_nets_ema.ckpt\n","    wget -N $URL -O $OUT_FILE\n","    \n","elif  [ $FILE == \"wing\" ]; then\n","    URL=https://www.dropbox.com/s/tjxpypwpt38926e/wing.ckpt?dl=0\n","    mkdir -p ./expr/checkpoints/\n","    OUT_FILE=./expr/checkpoints/wing.ckpt\n","    wget -N $URL -O $OUT_FILE\n","    URL=https://www.dropbox.com/s/91fth49gyb7xksk/celeba_lm_mean.npz?dl=0\n","    OUT_FILE=./expr/checkpoints/celeba_lm_mean.npz\n","    wget -N $URL -O $OUT_FILE\n","\n","elif  [ $FILE == \"celeba-hq-dataset\" ]; then\n","    URL=https://www.dropbox.com/s/f7pvjij2xlpff59/celeba_hq.zip?dl=0\n","    ZIP_FILE=./data/celeba_hq.zip\n","    mkdir -p ./data\n","    wget -N $URL -O $ZIP_FILE\n","    unzip $ZIP_FILE -d ./data\n","    rm $ZIP_FILE\n","\n","elif  [ $FILE == \"afhq-dataset\" ]; then\n","    URL=https://www.dropbox.com/s/t9l9o3vsx2jai3z/afhq.zip?dl=0\n","    ZIP_FILE=./data/afhq.zip\n","    mkdir -p ./data\n","    wget -N $URL -O $ZIP_FILE\n","    unzip $ZIP_FILE -d ./data\n","    rm $ZIP_FILE\n","\n","elif  [ $FILE == \"afhq-v2-dataset\" ]; then\n","    URL=https://www.dropbox.com/s/scckftx13grwmiv/afhq_v2.zip?dl=0\n","    ZIP_FILE=./data/afhq_v2.zip\n","    mkdir -p ./data\n","    wget -N $URL -O $ZIP_FILE\n","    unzip $ZIP_FILE -d ./data\n","    rm $ZIP_FILE\n","\n","else\n","    echo \"Available arguments are pretrained-network-celeba-hq, pretrained-network-afhq, celeba-hq-dataset, and afhq-dataset.\"\n","    exit 1\n","\n","fi"],"metadata":{"id":"T1vyuf7qCw8U"}},{"cell_type":"code","source":["sys.path"],"metadata":{"id":"XSW57rGWrXPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sys.path.insert(0, model_root)\n","sys.path"],"metadata":{"id":"OC4L4nme15hr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install munch"],"metadata":{"id":"LMF_oc0Tq7iz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OnIuQF-xp_QQ"},"outputs":[],"source":["import os\n","from munch import Munch\n","from collections import namedtuple\n","from copy import deepcopy\n","from functools import partial\n","from pathlib import Path\n","from itertools import chain\n","import time\n","import datetime\n","\n","# math\n","import math\n","import random\n","import numpy as np\n","\n","# pytorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils import data\n","from torch.utils.data.sampler import WeightedRandomSampler\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","\n","# image processing\n","import cv2\n","from PIL import Image\n","from skimage.filters import gaussian\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["os.environ['CUDA_VISIBLE_DEVICES']='4' # 사용할 GPU 번호를 설정합니다\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","img_size = 256\n","num_domains = 2 # Celeba-HQ(남자, 여자) / AFHQ dataset(강아지, 고양이, 야생동물)\n","latent_dim = 16 # latent code의 vector dimension 입니다.\n","hidden_dim = 512 \n","style_dim = 64"],"metadata":{"id":"cVVNO-AbqEup"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# AdaIN\n","class AdaIN(nn.Module):\n","    def __init__(self, style_dim, num_features):\n","        super().__init__()\n","        self.norm = nn.InstanceNorm2d(num_features, affine=False)\n","        self.fc = nn.Linear(style_dim, num_features*2)\n","\n","    def forward(self, x, s):\n","        h = self.fc(s)\n","        h = h.view(h.size(0), h.size(1), 1, 1)\n","        gamma, beta = torch.chunk(h, chunks=2, dim=1)\n","        return (1 + gamma) * self.norm(x) + beta"],"metadata":{"id":"0-VXxvy9qEsS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ResBlk\n","class ResBlk(nn.Module):\n","    def __init__(self, dim_in, dim_out, actv=nn.LeakyReLU(0.2), normalize=False, downsample=False):\n","        super().__init__()\n","        self.actv = actv\n","        self.normalize = normalize\n","        self.downsample = downsample\n","        self.learned_sc = dim_in != dim_out\n","        self._build_weights(dim_in, dim_out)\n","\n","    def _build_weights(self, dim_in, dim_out):\n","        self.conv1 = nn.Conv2d(dim_in, dim_in, 3, 1, 1)\n","        self.conv2 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)\n","        if self.normalize:\n","            self.norm1 = nn.InstanceNorm2d(dim_in, affine=True)\n","            self.norm2 = nn.InstanceNorm2d(dim_in, affine=True)\n","        if self.learned_sc:\n","            self.conv1x1 = nn.Conv2d(dim_in, dim_out, 1, 1, 0, bias=False)\n","\n","    def _shortcut(self, x):\n","        if self.learned_sc:\n","            x = self.conv1x1(x)\n","        if self.downsample:\n","            x = F.avg_pool2d(x, 2)\n","        return x\n","\n","    def _residual(self, x):\n","        if self.normalize:\n","            x = self.norm1(x)\n","        x = self.actv(x)\n","        x = self.conv1(x)\n","        if self.downsample:\n","            x = F.avg_pool2d(x, 2)\n","        if self.normalize:\n","            x = self.norm2(x)\n","        x = self.actv(x)\n","        x = self.conv2(x)\n","        return x\n","\n","    def forward(self, x):\n","        x = self._shortcut(x) + self._residual(x)\n","        return x / math.sqrt(2)  # unit variance"],"metadata":{"id":"e13a5LrwqEqu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# AdainResBlk\n","class AdainResBlk(nn.Module):\n","    def __init__(self, dim_in, dim_out, style_dim=64, actv=nn.LeakyReLU(0.2), upsample=False):\n","        super().__init__()\n","        self.actv = actv\n","        self.upsample = upsample\n","        self.learned_sc = dim_in != dim_out\n","        self._build_weights(dim_in, dim_out, style_dim)\n","\n","    def _build_weights(self, dim_in, dim_out, style_dim=64):\n","        self.conv1 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)\n","        self.conv2 = nn.Conv2d(dim_out, dim_out, 3, 1, 1)\n","        self.norm1 = AdaIN(style_dim, dim_in)\n","        self.norm2 = AdaIN(style_dim, dim_out)\n","        if self.learned_sc:\n","            self.conv1x1 = nn.Conv2d(dim_in, dim_out, 1, 1, 0, bias=False)\n","\n","    def _shortcut(self, x):\n","        if self.upsample:\n","            x = F.interpolate(x, scale_factor=2, mode='nearest')\n","        if self.learned_sc:\n","            x = self.conv1x1(x)\n","        return x\n","\n","    def _residual(self, x, s):\n","        x = self.norm1(x, s)\n","        x = self.actv(x)\n","        if self.upsample:\n","            x = F.interpolate(x, scale_factor=2, mode='nearest')\n","        x = self.conv1(x)\n","        x = self.norm2(x, s)\n","        x = self.actv(x)\n","        x = self.conv2(x)\n","        return x\n","\n","    def forward(self, x, s):\n","        out = self._residual(x, s)\n","        out = (out + self._shortcut(x)) / math.sqrt(2)\n","        return out"],"metadata":{"id":"hjyLFfGJqEoZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generator\n","class Generator(nn.Module):\n","    def __init__(self, img_size=256, style_dim=64, max_conv_dim=512):\n","        super().__init__()\n","        dim_in = 2**14 // img_size\n","        self.img_size = img_size\n","        self.from_rgb = nn.Conv2d(3, dim_in, 3, 1, 1)\n","        self.encode = nn.ModuleList()\n","        self.decode = nn.ModuleList()\n","        self.to_rgb = nn.Sequential(\n","            nn.InstanceNorm2d(dim_in, affine=True),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(dim_in, 3, 1, 1, 0))\n","\n","        # down/up-sampling blocks\n","        repeat_num = int(np.log2(img_size)) - 4 # img_size = 256이므로 repeat_num=4 가 됩니다.\n","        for _ in range(repeat_num):\n","            dim_out = min(dim_in*2, max_conv_dim)\n","            self.encode.append(\n","                ResBlk(dim_in, dim_out, normalize=True, downsample=True))\n","            self.decode.insert(\n","                0, AdainResBlk(dim_out, dim_in, style_dim, upsample=True))  # stack-like\n","            dim_in = dim_out\n","            \n","            # [1,2,3,4],[5,6,6,5], [4,3,2,1]\n","\n","        # bottleneck blocks\n","        for _ in range(2):\n","            self.encode.append(\n","                ResBlk(dim_out, dim_out, normalize=True))\n","            self.decode.insert(\n","                0, AdainResBlk(dim_out, dim_out, style_dim))\n","\n","\n","    def forward(self, x, s):\n","        x = self.from_rgb(x)\n","        cache = {}\n","        for block in self.encode:\n","            x = block(x)\n","        for block in self.decode:\n","            x = block(x, s)\n","        return self.to_rgb(x)"],"metadata":{"id":"hrtuzmX6qEkO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator = Generator(img_size, style_dim)"],"metadata":{"id":"ktWiB8wzqEgU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator"],"metadata":{"id":"d_nmsx_uqEef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mapping Network\n","class MappingNetwork(nn.Module):\n","    def __init__(self, latent_dim=16, style_dim=64, num_domains=2):\n","        super().__init__()\n","        layers = []\n","        layers += [nn.Linear(latent_dim, 512)]\n","        layers += [nn.ReLU()]\n","        for _ in range(3):\n","            layers += [nn.Linear(512, 512)]\n","            layers += [nn.ReLU()]\n","        self.shared = nn.Sequential(*layers)\n","\n","        self.unshared = nn.ModuleList()\n","        for _ in range(num_domains):\n","            self.unshared += [nn.Sequential(nn.Linear(512, 512),\n","                                            nn.ReLU(),\n","                                            nn.Linear(512, 512),\n","                                            nn.ReLU(),\n","                                            nn.Linear(512, 512),\n","                                            nn.ReLU(),\n","                                            nn.Linear(512, style_dim))]\n","\n","    def forward(self, z, y):\n","        h = self.shared(z)\n","        out = []\n","        for layer in self.unshared:\n","            out += [layer(h)]\n","        out = torch.stack(out, dim=1)  # (batch, num_domains, style_dim)\n","        idx = torch.LongTensor(range(y.size(0))).to(y.device)\n","        s = out[idx, y]  # (batch, style_dim) # y에 입력된 domain 에 해당하는 style 만 추출합니다.\n","        return s"],"metadata":{"id":"KQ-jM_pEqEb4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mapping_network = MappingNetwork(latent_dim, style_dim, num_domains)"],"metadata":{"id":"ZatKWGrmqEUW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mapping_network"],"metadata":{"id":"QXfsMbmbai8c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Style Encoder\n","class StyleEncoder(nn.Module):\n","    def __init__(self, img_size=256, style_dim=64, num_domains=2, max_conv_dim=512):\n","        super().__init__()\n","        dim_in = 2**14 // img_size\n","        blocks = []\n","        blocks += [nn.Conv2d(3, dim_in, 3, 1, 1)]\n","\n","        repeat_num = int(np.log2(img_size)) - 2\n","        for _ in range(repeat_num):\n","            dim_out = min(dim_in*2, max_conv_dim)\n","            blocks += [ResBlk(dim_in, dim_out, downsample=True)]\n","            dim_in = dim_out\n","\n","        blocks += [nn.LeakyReLU(0.2)]\n","        blocks += [nn.Conv2d(dim_out, dim_out, 4, 1, 0)]\n","        blocks += [nn.LeakyReLU(0.2)]\n","        self.shared = nn.Sequential(*blocks)\n","\n","        self.unshared = nn.ModuleList()\n","        for _ in range(num_domains):\n","            self.unshared += [nn.Linear(dim_out, style_dim)]\n","\n","    def forward(self, x, y):\n","        h = self.shared(x)\n","        h = h.view(h.size(0), -1)\n","        out = []\n","        for layer in self.unshared:\n","            out += [layer(h)]\n","        out = torch.stack(out, dim=1)  # (batch, num_domains, style_dim)\n","        idx = torch.LongTensor(range(y.size(0))).to(y.device)\n","        s = out[idx, y]  # (batch, style_dim)\n","        return s"],"metadata":{"id":"-okEcltUqESQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["style_encoder = StyleEncoder(img_size, style_dim, num_domains)"],"metadata":{"id":"P0R8DVbzqEQK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["style_encoder"],"metadata":{"id":"XTpNfq5QaotN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Discriminator\n","class Discriminator(nn.Module):\n","    def __init__(self, img_size=256, num_domains=2, max_conv_dim=512):\n","        super().__init__()\n","        dim_in = 2**14 // img_size\n","        blocks = []\n","        blocks += [nn.Conv2d(3, dim_in, 3, 1, 1)]\n","\n","        repeat_num = int(np.log2(img_size)) - 2\n","        for _ in range(repeat_num):\n","            dim_out = min(dim_in*2, max_conv_dim)\n","            blocks += [ResBlk(dim_in, dim_out, downsample=True)]\n","            dim_in = dim_out\n","\n","        blocks += [nn.LeakyReLU(0.2)]\n","        blocks += [nn.Conv2d(dim_out, dim_out, 4, 1, 0)]\n","        blocks += [nn.LeakyReLU(0.2)]\n","        blocks += [nn.Conv2d(dim_out, num_domains, 1, 1, 0)]\n","        self.main = nn.Sequential(*blocks)\n","\n","    def forward(self, x, y):\n","        out = self.main(x)\n","        out = out.view(out.size(0), -1)  # (batch, num_domains)\n","        idx = torch.LongTensor(range(y.size(0))).to(y.device)\n","        out = out[idx, y]  # (batch)\n","        return out"],"metadata":{"id":"yPtOJ0IqqEN0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["discriminator = Discriminator(img_size, num_domains)"],"metadata":{"id":"yacZVb1zqELf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["discriminator"],"metadata":{"id":"kH0A4IdDtb53"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nets = Munch(generator=generator,\n","             mapping_network=mapping_network,\n","             style_encoder=style_encoder,\n","             discriminator=discriminator)\n","\n","print(nets.keys())"],"metadata":{"id":"kNMi9kvazf0q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nets"],"metadata":{"id":"CeiSLRn2tekt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !bash download.sh afhq-dataset\n","# !bash download.sh celeba-hq-dataset"],"metadata":{"id":"gQwRGPgUzfuZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 전처리\n","# 폴더의 이미지 목록을 추출\n","def listdir(dname):\n","    fnames = list(chain(*[list(Path(dname).rglob('*.' + ext))\n","                          for ext in ['png', 'jpg', 'jpeg', 'JPG']]))\n","    return fnames\n","\n","\n","# 입력 이미지 (source image) 데이터셋 구성\n","class DefaultDataset(data.Dataset):\n","    def __init__(self, root, transform=None):\n","        self.samples = listdir(root)\n","        self.samples.sort()\n","        self.transform = transform\n","        self.targets = None\n","\n","    def __getitem__(self, index):\n","        fname = self.samples[index]\n","        img = Image.open(fname).convert('RGB')\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        return img\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","\n","# reference 이미지 데이터셋 구성\n","class ReferenceDataset(data.Dataset):\n","    def __init__(self, root, transform=None):\n","        self.samples, self.targets = self._make_dataset(root)\n","        self.transform = transform\n","\n","    def _make_dataset(self, root):\n","        domains = os.listdir(root)\n","        fnames, fnames2, labels = [], [], []\n","        for idx, domain in enumerate(sorted(domains)):\n","            class_dir = os.path.join(root, domain)\n","            cls_fnames = listdir(class_dir)\n","            fnames += cls_fnames\n","            fnames2 += random.sample(cls_fnames, len(cls_fnames))\n","            labels += [idx] * len(cls_fnames)\n","        return list(zip(fnames, fnames2)), labels\n","\n","    def __getitem__(self, index):\n","        fname, fname2 = self.samples[index]\n","        label = self.targets[index]\n","        img = Image.open(fname).convert('RGB')\n","        img2 = Image.open(fname2).convert('RGB')\n","        if self.transform is not None:\n","            img = self.transform(img)\n","            img2 = self.transform(img2)\n","        return img, img2, label\n","\n","    def __len__(self):\n","        return len(self.targets)\n","\n","\n","# 일부 클래스에 치중되지 않도록 밸런싱 sampler\n","def _make_balanced_sampler(labels):\n","    class_counts = np.bincount(labels)\n","    class_weights = 1. / class_counts\n","    weights = class_weights[labels]\n","    return WeightedRandomSampler(weights, len(weights))\n","\n"," \n","# 학습용 데이터셋 구성 \n","def get_train_loader(root, which='source', img_size=256, batch_size=8, prob=0.5, num_workers=4):\n","    print('Preparing DataLoader to fetch %s images '\n","          'during the training phase...' % which)\n","\n","    crop = transforms.RandomResizedCrop(img_size, scale=[0.8, 1.0], ratio=[0.9, 1.1])\n","    rand_crop = transforms.Lambda(lambda x: crop(x) if random.random() < prob else x) \n","    transform = transforms.Compose([\n","        rand_crop, # 일정 확률로 이미지를 random crop\n","        transforms.Resize([img_size, img_size]),\n","        transforms.RandomHorizontalFlip(), # 일정 확률로 이미지를 좌우반전\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","                             std=[0.5, 0.5, 0.5]),\n","    ])\n","\n","    if which == 'source':\n","        dataset = ImageFolder(root, transform)\n","    elif which == 'reference':\n","        dataset = ReferenceDataset(root, transform)\n","    else:\n","        raise NotImplementedError\n","\n","    sampler = _make_balanced_sampler(dataset.targets)\n","    return data.DataLoader(dataset=dataset,\n","                           batch_size=batch_size,\n","                           sampler=sampler,\n","                           num_workers=num_workers,\n","                           pin_memory=True, # pin_memory = True 인 경우, cpu에서 gpu로 텐서를 이동시킬 때 유리\n","                           drop_last=True) # 마지막 batch의 경우 남는 데이터의 수에 따라 shape가 맞지 않을 수 있으므로 제외\n","\n","\n","# 평가용 데이터셋 \n","def get_test_loader(root, img_size=256, batch_size=32, shuffle=True, num_workers=4):\n","    print('Preparing DataLoader for the generation phase...')\n","    transform = transforms.Compose([\n","        transforms.Resize([img_size, img_size]),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","                             std=[0.5, 0.5, 0.5]),\n","    ])\n","\n","    dataset = ImageFolder(root, transform)\n","    return data.DataLoader(dataset=dataset,\n","                           batch_size=batch_size,\n","                           shuffle=shuffle,\n","                           num_workers=num_workers,\n","                           pin_memory=True)\n","\n","# InputFetcher 정의\n","class InputFetcher:\n","    def __init__(self, loader, loader_ref=None, latent_dim=16, mode=''):\n","        self.loader = loader\n","        self.loader_ref = loader_ref\n","        self.latent_dim = latent_dim\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.mode = mode\n","\n","    def _fetch_inputs(self):\n","        try:\n","            x, y = next(self.iter)\n","        except (AttributeError, StopIteration):\n","            self.iter = iter(self.loader)\n","            x, y = next(self.iter)\n","        return x, y\n","\n","    def _fetch_refs(self):\n","        try:\n","            x, x2, y = next(self.iter_ref)\n","        except (AttributeError, StopIteration):\n","            self.iter_ref = iter(self.loader_ref)\n","            x, x2, y = next(self.iter_ref)\n","        return x, x2, y\n","\n","    def __next__(self):\n","        x, y = self._fetch_inputs()\n","        if self.mode == 'train':\n","            x_ref, x_ref2, y_ref = self._fetch_refs()\n","            z_trg = torch.randn(x.size(0), self.latent_dim)\n","            z_trg2 = torch.randn(x.size(0), self.latent_dim)\n","            inputs = Munch(x_src=x, y_src=y, y_ref=y_ref,\n","                           x_ref=x_ref, x_ref2=x_ref2,\n","                           z_trg=z_trg, z_trg2=z_trg2)\n","        elif self.mode == 'val':\n","            x_ref, y_ref = self._fetch_inputs()\n","            inputs = Munch(x_src=x, y_src=y,\n","                           x_ref=x_ref, y_ref=y_ref)\n","        elif self.mode == 'test':\n","            inputs = Munch(x=x, y=y)\n","        else:\n","            raise NotImplementedError\n","\n","        return Munch({k: v.to(self.device)\n","                      for k, v in inputs.items()})"],"metadata":{"id":"E86I3pgy1YAG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_img_dir = '/content/drive/MyDrive/GAN/starGAN_v2/data/celeba_hq/train'\n","val_img_dir = '/content/drive/MyDrive/GAN/starGAN_v2/data/celeba_hq/val'\n","\n","randcrop_prob = 0.5\n","num_workers = 2\n","\n","batch_size = 1\n","val_batch_size = 1"],"metadata":{"id":"hMxaggc41X8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaders = Munch(src=get_train_loader(root=train_img_dir,\n","                                             which='source',\n","                                             img_size=img_size,\n","                                             batch_size=batch_size,\n","                                             prob=randcrop_prob,\n","                                             num_workers=num_workers),\n","                        ref=get_train_loader(root=train_img_dir,\n","                                             which='reference',\n","                                             img_size=img_size,\n","                                             batch_size=batch_size,\n","                                             prob=randcrop_prob,\n","                                             num_workers=num_workers),\n","                        val=get_test_loader(root=val_img_dir,\n","                                            img_size=img_size,\n","                                            batch_size=val_batch_size,\n","                                            shuffle=True,\n","                                            num_workers=num_workers))"],"metadata":{"id":"hrl54Rnm1X5l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fetcher = InputFetcher(loaders.src, loaders.ref, latent_dim, 'train')\n","fetcher_val = InputFetcher(loaders.val, None, latent_dim, 'val')"],"metadata":{"id":"ZLLF9cjb8MFq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = next(fetcher)"],"metadata":{"id":"R2sfS2dd8L-9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs.keys()"],"metadata":{"id":"pxaTIvHc8L8i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for key in inputs.keys():\n","    print(key, inputs[key].shape)"],"metadata":{"id":"TQpNPkBh8L6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs_val = next(fetcher_val)"],"metadata":{"id":"xGRJBjsZ-HiQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs_val.keys()"],"metadata":{"id":"ZZctk7Oq-Hxc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for key in inputs_val.keys():\n","    print(key, inputs_val[key].shape)"],"metadata":{"id":"dCGKTTvP-Huw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_src_ex = inputs.x_src[0].permute(1,2,0).data.cpu().numpy() # 이미지로 보여주기 위해서는 \n","x_ref_ex = inputs.x_ref[0].permute(1,2,0).data.cpu().numpy() # (3, 256, 256) -> (256, 256, 3)으로 변경시켜주어야합니다.\n","x_ref2_ex = inputs.x_ref2[0].permute(1,2,0).data.cpu().numpy()\n","\n","plt.figure(figsize=(20,7))\n","plt.subplot(1,3,1)\n","plt.imshow(x_src_ex*0.5+0.5) # -1~1 로 normalize 된 이미지를 0~1로 복구시킵니다.\n","plt.title('x_src')\n","plt.subplot(1,3,2)\n","plt.imshow(x_ref_ex*0.5+0.5)\n","plt.title('x_ref')\n","plt.subplot(1,3,3)\n","plt.imshow(x_ref2_ex*0.5+0.5)\n","plt.title('x_ref2')\n","plt.show()"],"metadata":{"id":"hboLoE7n-Hsd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# optimizer\n","optims = Munch()\n","for net in nets.keys():\n","    optims[net] = torch.optim.Adam(params=nets[net].parameters(),\n","                                    lr=1e-6 if net == 'mapping_network' else 1e-4,\n","                                    betas=[0.0, 0.99],\n","                                    weight_decay=1e-4)"],"metadata":{"id":"M1sHYCrL-Hqa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# logit\n","def adv_loss(logits, target):\n","    assert target in [1, 0]\n","    targets = torch.full_like(logits, fill_value=target)\n","    loss = F.binary_cross_entropy_with_logits(logits, targets)\n","    return loss"],"metadata":{"id":"Q74_JvZ5-HpB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# zero-centered gradient penalty\n","def r1_reg(d_out, x_in):\n","    batch_size = x_in.size(0)\n","    grad_dout = torch.autograd.grad(\n","        outputs=d_out.sum(), inputs=x_in,\n","        create_graph=True, retain_graph=True, only_inputs=True\n","    )[0]\n","    grad_dout2 = grad_dout.pow(2)\n","    assert(grad_dout2.size() == x_in.size())\n","    reg = 0.5 * grad_dout2.view(batch_size, -1).sum(1).mean(0)\n","    return reg"],"metadata":{"id":"MwIrIkz--Hms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# discrimitor loss\n","def compute_d_loss(nets, x_real, y_org, y_trg, z_trg=None, x_ref=None):\n","    assert (z_trg is None) != (x_ref is None)\n","    \n","    # with real images\n","    x_real.requires_grad_() # x_real이 gradient가 흐를 수 있도록 바꿈\n","    out = nets.discriminator(x_real, y_org) # 실제 입력과 실제 입력데이터의 domain 정보를 입력\n","    loss_real = adv_loss(out, 1) # 예측 결과값이 1이 되도록 학습\n","    loss_reg = r1_reg(out, x_real) # gradient penalty를 적용\n","\n","    # with fake images\n","    with torch.no_grad():\n","        if z_trg is not None:\n","            s_trg = nets.mapping_network(z_trg, y_trg) # 입력된 target domian과 latent code로 style 생성\n","        else:  # x_ref is not None\n","            s_trg = nets.style_encoder(x_ref, y_trg) # 입력된 target domain과 reference image로 style 생성\n","\n","        x_fake = nets.generator(x_real, s_trg) # 입력 이미지 (source image)와 style로부터 fake image 생성\n","    out = nets.discriminator(x_fake, y_trg) # fake image와 target domain 을 입력하여 discriminator가 분류\n","    loss_fake = adv_loss(out, 0) # discriminator가 fake image를 볼 때에는 0을 예측하도록 학습\n","\n","    loss = loss_real + loss_fake + loss_reg # loss sum\n","    return loss, Munch(real=loss_real.item(),\n","                       fake=loss_fake.item(),\n","                       reg=loss_reg.item())"],"metadata":{"id":"9yLJ3xJ--Hkm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generator loss\n","def compute_g_loss(nets, x_real, y_org, y_trg, z_trgs=None, x_refs=None):\n","    assert (z_trgs is None) != (x_refs is None)\n","    if z_trgs is not None:\n","        z_trg, z_trg2 = z_trgs\n","    if x_refs is not None:\n","        x_ref, x_ref2 = x_refs\n","\n","    # adversarial loss\n","    if z_trgs is not None:\n","        s_trg = nets.mapping_network(z_trg, y_trg)\n","    else:\n","        s_trg = nets.style_encoder(x_ref, y_trg)\n","\n","    x_fake = nets.generator(x_real, s_trg)\n","    out = nets.discriminator(x_fake, y_trg)\n","    loss_adv = adv_loss(out, 1)\n","\n","    # style reconstruction loss\n","    s_pred = nets.style_encoder(x_fake, y_trg)\n","    loss_sty = torch.mean(torch.abs(s_pred - s_trg))\n","\n","    # diversity sensitive loss\n","    if z_trgs is not None:\n","        s_trg2 = nets.mapping_network(z_trg2, y_trg)\n","    else:\n","        s_trg2 = nets.style_encoder(x_ref2, y_trg)\n","    x_fake2 = nets.generator(x_real, s_trg2)\n","    x_fake2 = x_fake2.detach()\n","    loss_ds = torch.mean(torch.abs(x_fake - x_fake2))\n","\n","    # cycle-consistency loss\n","    s_org = nets.style_encoder(x_real, y_org)\n","    x_rec = nets.generator(x_fake, s_org)\n","    loss_cyc = torch.mean(torch.abs(x_rec - x_real))\n","\n","    loss = loss_adv +loss_sty - loss_ds + loss_cyc\n","    return loss, Munch(adv=loss_adv.item(),\n","                       sty=loss_sty.item(),\n","                       ds=loss_ds.item(),\n","                       cyc=loss_cyc.item())"],"metadata":{"id":"pZKP4A6B_nNK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show images\n","def show_image(nets, inputs, latent_dim=16):\n","    x_src, y_src = inputs.x_src, inputs.y_src\n","    x_ref, y_ref = inputs.x_ref, inputs.y_ref\n","\n","    device = inputs.x_src.device\n","    \n","    # reference-guided image synthesis & cycle consistency\n","    N, C, H, W = x_src.size()\n","    s_ref = nets.style_encoder(x_ref, y_ref)\n","    x_fake = nets.generator(x_src, s_ref)\n","    s_src = nets.style_encoder(x_src, y_src)\n","    x_rec = nets.generator(x_fake, s_src)\n","    \n","    plt.figure(figsize=(20,7))\n","    plt.subplot(1,4,1)\n","    plt.imshow(x_src.data.cpu()[0].permute(1,2,0)*0.5+0.5)\n","    plt.title('x_src')\n","    plt.subplot(1,4,2)\n","    plt.imshow(x_ref.data.cpu()[0].permute(1,2,0)*0.5+0.5)\n","    plt.title('x_ref')\n","    plt.subplot(1,4,3)\n","    plt.imshow(x_fake.data.cpu()[0].permute(1,2,0).clamp(min=-1,max=1)*0.5+0.5)\n","    plt.title('x_fake')\n","    plt.subplot(1,4,4)\n","    plt.imshow(x_rec.data.cpu()[0].permute(1,2,0).clamp(min=-1,max=1)*0.5+0.5)\n","    plt.title('x_rec')\n","    plt.show()    \n","    \n","    # latent-guided image synthesis\n","    n_out = 3\n","    \n","    plt.figure(figsize=(20,15))\n","    for i in range(3):\n","        plt.subplot(n_out,4,1+4*i)\n","        plt.imshow(x_src.data.cpu()[0].permute(1,2,0)*0.5+0.5)\n","        plt.title('x_src')\n","    \n","    y_trg_list = [torch.tensor(y).repeat(N).to(device) for y in range(2)] # celeba-hq 2, afhq 3\n","    z_trg_list = torch.randn(n_out, 1, latent_dim).repeat(1, N, 1).to(device)\n","    N, C, H, W = x_src.size()\n","    latent_dim = z_trg_list[0].size(1)\n","\n","    for i, y_trg in enumerate(y_trg_list):\n","        n = 0\n","        z_many = torch.randn(10000, latent_dim).to(x_src.device)\n","        y_many = torch.LongTensor(10000).to(x_src.device).fill_(y_trg[0])\n","        s_many = nets.mapping_network(z_many, y_many)\n","        s_avg = torch.mean(s_many, dim=0, keepdim=True)\n","        s_avg = s_avg.repeat(N, 1)\n","\n","        for z_trg in z_trg_list:\n","            n+=1\n","            s_trg = nets.mapping_network(z_trg, y_trg)\n","            x_fake = nets.generator(x_src, s_trg)\n","\n","            plt.subplot(n_out,4,n+i*4+1)\n","            plt.imshow(x_fake.data.cpu()[0].permute(1,2,0).clamp(min=-1,max=1)*0.5+0.5)\n","            plt.title('x_fake-{}'.format(n))\n","    plt.show()  "],"metadata":{"id":"Vfb7xocr-Dl0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# to gpu setting for training\n","for key in nets.keys():\n","    nets[key] = nets[key].to(device) #.cuda() 와 같음\n","    nets[key].train()\n","\n","# set iteration\n","total_iters = 100000\n","\n","# training(필요 시)\n","start_time = time.time()\n","for i in range(total_iters):\n","    # fetch images and labels\n","    inputs = next(fetcher)\n","    x_real, y_org = inputs.x_src, inputs.y_src\n","    x_ref, x_ref2, y_trg = inputs.x_ref, inputs.x_ref2, inputs.y_ref\n","    z_trg, z_trg2 = inputs.z_trg, inputs.z_trg2\n","    \n","    \"\"\"\n","    latent code를 입력한 경우와 reference image를 입력한 경우를 각각 따로 학습\n","    \"\"\"\n","    # mapping net(latent code)에 대한 discriminator 학습\n","    d_loss, d_losses_latent = compute_d_loss(\n","        nets, x_real, y_org, y_trg, z_trg=z_trg)\n","    for optim in optims.values():\n","        optim.zero_grad()\n","    d_loss.backward()\n","    optims.discriminator.step()\n","\n","    # reference image에 대한 discriminator 학습\n","    d_loss, d_losses_ref = compute_d_loss(\n","        nets, x_real, y_org, y_trg, x_ref=x_ref)\n","    for optim in optims.values():\n","        optim.zero_grad()\n","    d_loss.backward()\n","    optims.discriminator.step()\n","\n","    \"\"\"\n","    discriminator와 마찬가지로, latent code를 입력한 경우와 reference image를 입력한 경우를 따로 학습\n","    \"\"\"\n","    # mapping net(latent code)에 대한 generator 학습\n","    g_loss, g_losses_latent = compute_g_loss(\n","        nets, x_real, y_org, y_trg, z_trgs=[z_trg, z_trg2])\n","    for optim in optims.values():\n","        optim.zero_grad()\n","    g_loss.backward()\n","    optims.generator.step()\n","    optims.mapping_network.step()\n","    optims.style_encoder.step()\n","\n","    # reference image에 대한 generator 학습\n","    g_loss, g_losses_ref = compute_g_loss(\n","        nets, x_real, y_org, y_trg, x_refs=[x_ref, x_ref2])\n","    for optim in optims.values():\n","        optim.zero_grad()\n","    g_loss.backward()\n","    optims.generator.step()\n","\n","    # print out log info\n","    if (i+1) % 10 == 0:\n","        #log \n","        elapsed = time.time() - start_time\n","        elapsed = str(datetime.timedelta(seconds=elapsed))[:-7]\n","        log = \"[%s], iter [%i/%i], \" % (elapsed, i+1, total_iters)\n","        all_losses = dict()\n","        for loss, prefix in zip([d_losses_latent, d_losses_ref, g_losses_latent, g_losses_ref],\n","                                ['D/latent_', 'D/ref_', 'G/latent_', 'G/ref_']):\n","            for key, value in loss.items():\n","                all_losses[prefix + key] = value\n","        log += ' '.join(['%s: [%.4f]' % (key, value) for key, value in all_losses.items()])\n","        print(log)\n","        \n","        \n","    if (i+1) % 100 == 0:\n","        # result\n","        show_image(nets, inputs_val)"],"metadata":{"id":"n0qoyiT8-Dfm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !bash download.sh pretrained-network-celeba-hq\n","# !bash download.sh pretrained-network-afhq\n","# !bash download.sh wing"],"metadata":{"id":"ta54RuAV-Dc-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model load\n","ckpt_path = '/content/drive/MyDrive/GAN/starGAN_v2/expr/checkpoints/celeba_hq/100000_nets_ema.ckpt' \n","ckpt = torch.load(ckpt_path, map_location=torch.device('cpu')) # gpu 사용 시, map_location=torch.device('cpu') 삭제"],"metadata":{"id":"dv13Ik5m-Dan"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 참고 afhq\n","ckpt_path_afhq = '/content/drive/MyDrive/GAN/starGAN_v2/expr/checkpoints/afhq/100000_nets_ema.ckpt' \n","ckpt_afhq = torch.load(ckpt_path, map_location=torch.device('cpu')) # gpu 사용 시, map_location=torch.device('cpu') 삭제"],"metadata":{"id":"6gxTc9dWO3CG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ckpt.keys()"],"metadata":{"id":"k7NIMpNNA-i0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ckpt_celeba_li = {}\n","for key in ckpt['generator'].keys():\n","    ckpt_celeba_li[key] = ckpt['generator'][key].shape"],"metadata":{"id":"4_T_vKJzMsX7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ckpt_afhq_li = {}\n","for key in ckpt_afhq['generator'].keys():\n","    ckpt_afhq_li[key] = ckpt_afhq['generator'][key].shape"],"metadata":{"id":"CPhTDDi6N4Zy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ckpt_celeba_li == ckpt_afhq_li"],"metadata":{"id":"PTrhA7OGc79a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for key in ckpt.keys():\n","    if key == 'fan':\n","        continue\n","    nets[key].load_state_dict(ckpt[key], strict=False)\n","    \n","for net in nets.values():\n","    net.eval()\n","    #net.cuda()"],"metadata":{"id":"IV7Xw0C-BBGl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test\n","show_image(nets, inputs_val)"],"metadata":{"id":"3EvEMjTqBCFi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start_time = time.time()\n","for i in range(total_iters):\n","    # fetch images and labels\n","    inputs = next(fetcher_val)\n","    show_image(nets, inputs)\n","    print('\\n\\n')"],"metadata":{"id":"ZQg_6ljzBCPA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"rcj8vm96BCYX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"OHCS2pbjBCf9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"6hX_qrirBCn_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"UqucPqWmBCuw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"O-9XvLpWBC2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"qRWedY_lBC95"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Lj8yXJKPBDE9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"zJn1ZpfoBDMS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"8h63CnvxBDTa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"od_UVr62BDZ4"},"execution_count":null,"outputs":[]}]}